{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"090a-autoencoder_colorize_V0.2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1AC43X-BvAslsvNv5fSPlGmPTE4AGVAbB","authorship_tag":"ABX9TyOL0wYAIs6yo/ddHK4n+Ykn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# https://www.youtube.com/watch?v=EujccFRio7o\n","\n","\"\"\"\n","Unable to upload lots of images to Github.\n","Create your own set of images by downloading from Google. \n","\"\"\""],"metadata":{"id":"n8nBMuqrtnjc"}},{"cell_type":"code","source":["#!pip install google_images_download"],"metadata":{"id":"5kWGQUqhum36"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT4z-6Buti8a","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1661195237449,"user_tz":-180,"elapsed":11,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"513c4acf-7118-425d-d122-487b4586562d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'from google_images_download import google_images_download\\nresponse=google_images_download.googleimagesdownload()\\narguments={\"keywords\":\"national park, dog park\", \"limit\":100,\"print_urls\":False}\\npaths=response.download(arguments)\\n\\nprint(paths)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["'''from google_images_download import google_images_download\n","response=google_images_download.googleimagesdownload()\n","arguments={\"keywords\":\"national park, dog park\", \"limit\":100,\"print_urls\":False}\n","paths=response.download(arguments)\n","\n","print(paths)'''"]},{"cell_type":"code","source":["#!wget https://valuestockphoto.com/freehighresimages/sample_cart.zip -P downloads"],"metadata":{"id":"EMxosqxVvGYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip downloads/sample_cart.zip"],"metadata":{"id":"ovXdeKZQueHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.layers import Conv2D, UpSampling2D\n","from keras.models import Sequential\n","from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","from skimage.color import rgb2lab, lab2rgb\n","from skimage.transform import resize\n","from skimage.io import imsave\n","import numpy as np\n","import tensorflow as tf\n","from skimage.io import imsave, imshow"],"metadata":{"id":"GRH1rIs6xTWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resimlerin path klasöründe değil onun alt klasöründe olması gerek\n","path = 'drive/MyDrive/data/resimler'\n","\n","#Normalize images - divide by 255\n","train_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","#Resize images, if needed\n","train = train_datagen.flow_from_directory(path, \n","                                          target_size=(256, 256), \n","                                          batch_size=340, \n","                                          class_mode=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ij3Df5pRf1fV","executionInfo":{"status":"ok","timestamp":1661195243280,"user_tz":-180,"elapsed":2356,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"b0c25828-7b05-4383-f208-a4a9e9d5e5c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 355 images belonging to 1 classes.\n"]}]},{"cell_type":"code","source":["#Convert from RGB to Lab\n","\"\"\"\n","by iterating on each image, we convert the RGB to Lab. \n","Think of LAB image as a grey image in L channel and all color info stored in A and B channels. \n","The input to the network will be the L channel, so we assign L channel to X vector. \n","And assign A and B to Y.\n","\"\"\"\n","\n","X =[]\n","Y =[]\n","for img in train[0]:\n","  try:\n","      lab = rgb2lab(img)\n","      X.append(lab[:,:,0]) \n","      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n","      #so we divide the values by 128 to restrict values to between -1 and 1.\n","  except:\n","     print('error')\n","X = np.array(X)\n","Y = np.array(Y)\n","X = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\n","print(X.shape)\n","print(Y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_C7GLQEogPeB","executionInfo":{"status":"ok","timestamp":1661195261840,"user_tz":-180,"elapsed":18565,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"40b6ab34-39b9-4bab-b6af-78bc680ebe69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(340, 256, 256, 1)\n","(340, 256, 256, 2)\n"]}]},{"cell_type":"code","source":["#Encoder\n","\n","model = Sequential()\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n","model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n","\n","#Decoder\n","#Decoder\n","#Note: For the last layer we use tanh instead of Relu. \n","#This is because we are colorizing the image in this layer using 2 filters, A and B.\n","#A and B values range between -1 and 1 so tanh (or hyperbolic tangent) is used\n","#as it also has the range between -1 and 1. \n","#Other functions go from 0 to 1.\n","model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n","model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n","model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])"],"metadata":{"id":"AtKsHmFSggbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiVdLgUHj6jp","executionInfo":{"status":"ok","timestamp":1661195262413,"user_tz":-180,"elapsed":18,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"1ce99a8f-420c-4b43-dab5-5bca1db4c3e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 128, 128, 64)      640       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 128, 128, 128)     73856     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 64, 64, 128)       147584    \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 64, 64, 256)       295168    \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 32, 32, 256)       590080    \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 32, 32, 512)       1180160   \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 32, 32, 512)       2359808   \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 32, 32, 256)       1179904   \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 32, 32, 128)       295040    \n","                                                                 \n"," up_sampling2d (UpSampling2D  (None, 64, 64, 128)      0         \n"," )                                                               \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 64, 64, 64)        73792     \n","                                                                 \n"," up_sampling2d_1 (UpSampling  (None, 128, 128, 64)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 128, 128, 32)      18464     \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 128, 128, 16)      4624      \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 128, 128, 2)       290       \n","                                                                 \n"," up_sampling2d_2 (UpSampling  (None, 256, 256, 2)      0         \n"," 2D)                                                             \n","                                                                 \n","=================================================================\n","Total params: 6,219,410\n","Trainable params: 6,219,410\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["\n","model.fit(X,Y,validation_split=0.1, epochs=5, batch_size=16)\n","\n","model.save('other_files/colorize_autoencoder.model')\n","\n","#epoch 0 girildiği için aslında eğitim yapılmıyor. En az 300 epoch eğitim gerekli\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"LCcsGeRrkEUj","executionInfo":{"status":"ok","timestamp":1661195262414,"user_tz":-180,"elapsed":9,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"936ca608-bbe7-43ec-e0b6-e7df4fd78b32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nmodel.fit(X,Y,validation_split=0.1, epochs=0, batch_size=16)\\n\\nmodel.save('other_files/colorize_autoencoder.model')\\n\\n#epoch 0 girildiği için aslında eğitim yapılmıyor. En az 300 epoch eğitim gerekli\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["\n","###########################################################\n","#Load saved model and test on images.\n","#colorize_autoencoder300.model is trained on 300 epochs\n","#\n","#\n","#aşağıda 300 epoch eğitilmiş ve kaydedilmiş model yüklenip kullanılmıştır.\n","#eğitilmiş modeli çalıştırmak için model nesnesini hiç oluşturmamış olmak gerekiyor\n","'''\n","tf.keras.models.load_model(\n","    'drive/MyDrive/Colab Notebooks/ImageProcessing/AutoEncoders/colorize_autoencoder.model',\n","    custom_objects=None,\n","    compile=True)\n","\n","'''\n","\n","\n","img1_color=[]\n","img1=img_to_array(load_img('drive/MyDrive/data/resimler/manzara/016.jpg'))\n","img1 = resize(img1 ,(256,256))\n","img1_color.append(img1)\n","img1_color = np.array(img1_color, dtype=float)\n","img1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]\n","img1_color = img1_color.reshape(img1_color.shape+(1,))\n","output1 = model.predict(img1_color)\n","output1 = output1*128\n","result = np.zeros((256, 256, 3))\n","result[:,:,0] = img1_color[0][:,:,0]\n","result[:,:,1:] = output1[0]\n","imsave(\"result.png\", lab2rgb(result))\n","imshow(lab2rgb(result))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"UTURoMFfkKoh","executionInfo":{"status":"error","timestamp":1661195356771,"user_tz":-180,"elapsed":757,"user":{"displayName":"Yusuf Bilgen","userId":"05683862783259875968"}},"outputId":"33e9f310-8c3b-4082-a2b5-2e370dbfddb2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-a663a4986c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m '''\n\u001b[0;32m---> 14\u001b[0;31m model.load_model('drive/MyDrive/Colab Notebooks/ImageProcessing/AutoEncoders/colorize_autoencoder.model',\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     compile=True)\n","\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'load_model'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Hd9BDcqirnyr"},"execution_count":null,"outputs":[]}]}