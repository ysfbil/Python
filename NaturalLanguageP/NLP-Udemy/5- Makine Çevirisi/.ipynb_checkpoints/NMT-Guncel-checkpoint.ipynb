{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding # CuDNNGRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss ' #başlangıç kelimesi. Bu kelime metinde geçmeyen bir kelime olmalıdır\n",
    "mark_end = ' eeee' #bitiş kelimesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = []\n",
    "data_dest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dosyadaki türkçe ve ingilizce kelimeleri ayırıyoruz. İngilizce source, Tükçe destination olarak belirlendi\n",
    "\n",
    "for line in open('tur.txt', encoding='UTF-8'):\n",
    "    en_text, tr_text = line.rstrip().split('\\t')\n",
    "    \n",
    "    tr_text = mark_start + tr_text + mark_end #türkçe hedef seti olduğu için her cümleye başlangıç ve bitiş ekleniyor\n",
    "    \n",
    "    data_src.append(en_text)\n",
    "    data_dest.append(tr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It may seem like Tom has forgotten all about the matter, but deep down inside, he's still torn up about it.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Tom mesele hakkında her şeyi unutmuş gibi görünebilir fakat hâlâ gerçekten onun hakkında kendini harap ediyor. eeee'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The other day when I stopped by at a friend's house, it wasn't my friend that came out of the front door, but her husband.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Geçenlerde bir arkadaşımın evinin yanında durduğumda, ön kapıdan çıkan arkadaşım değil fakat kocasıydı. eeee'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "        \n",
    "        self.fit_on_texts(texts) #kelime dictionary oluşturuyoruz\n",
    "        \n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys())) #kelime dict. tersini oluşturduk\n",
    "        \n",
    "        self.tokens = self.texts_to_sequences(texts) #kelimeleri tokenlaştırdık\n",
    "        \n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens] #ters işaretli ise listeyi tersten oluşturuyoruz\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "            \n",
    "        self.num_tokens = [len(x) for x in self.tokens] #token uzunluk dizisi oluşturduk\n",
    "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens) #max token uzunluğunu ortalam ve standart sapmadan hesapladık\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        \n",
    "        self.tokens_padded = pad_sequences(self.tokens, #padding işlemi yaparak max'tan kısa veya uzun olanları hizalayıp tüm token uzunluklarını eşitledik\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "        \n",
    "    def token_to_word(self, token): #tokenları kelimeye çeviren fonksiyon\n",
    "        word = ' ' if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "    \n",
    "    def tokens_to_string(self, tokens): #tokenlardan metin elde ediyoruz\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        text = ' '.join(words)\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, padding, reverse=False): #metinden tokenlar çıkartıyoruz\n",
    "        tokens = self.texts_to_sequences([text]) #cümleyi tokenlarına ayırıyoruz\n",
    "        tokens = np.array(tokens) \n",
    "        \n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1) #reverse ise tokenları ters çevirdik(axis 1 satır işlemi için)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "            \n",
    "        tokens = pad_sequences(tokens,\n",
    "                               maxlen=self.max_tokens,\n",
    "                               padding=padding,\n",
    "                               truncating=truncating)\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = TokenizerWrap(texts=data_src, #ingilizce metin tanınıp anlam vektörü oluşturulacağı için (encoding)\n",
    "                              padding='pre', #padding pre yani kısa olan cümlelerin başı 0 ile dolduruluyor\n",
    "                              reverse=True, #böylece y.sinir ağı sonrakileri daha etkili aldığından bu işlem yapılıyor\n",
    "                              num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dest = TokenizerWrap(texts=data_dest, #türkçe metin ise gelen ingizceden türetilmiş anlam vektöründen \n",
    "                              padding='post', #yorumlandığı için daha kısa olan türkçe kelimelerin sonu 0 ile pad ediliyor (post)\n",
    "                              reverse=False, #çünkü bu işlemin baş kısmı daha önemli\n",
    "                              num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 31)\n",
      "(303, 25)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded #pad edilmiş tokenlar olarak değişkenleri belirliyoruz\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 204, 318, 697, 698, 699,  14,  35,  15, 700,   6,  87, 701,\n",
       "         2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss amerikan haber irene kasırgası'nın avrupa kadar büyük olduğunu bildiriyor bu biraz abartıdır eeee\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 765,  39,\n",
       "         4, 311,   3,  10, 140, 764,  29, 438,  29,  10, 763, 762,   7,\n",
       "       761,  10, 437, 310,   1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exaggeration an of bit a is which europe as big as is irene hurricane that reporting is news american the'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The other day when I stopped by at a friend's house, it wasn't my friend that came out of the front door, but her husband.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = tokens_src #encoder giriş için ingilizce tokenlar seçildi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = tokens_dest[:, :-1] #dekoder girişi için türkçe tokenlar seçildi\n",
    "decoder_output_data = tokens_dest[:, 1:] #dekoder çıkışı olarak ise türkçe tokenların bir basamak sonrası seçildi\n",
    "#böylece y.sinir ağı gelen türkçe kelimeden sonra gelecek kelimeyi ingilizce y.sinir ağından gelen anlam vektörü ile birlikte\n",
    "#yorumlayıp bulmaya çalışacak. Yani her defasında \n",
    "# (ing. anlam vektörü)+(türkçe kelime)=(türkçe sonraki kelime) şeklinde tahminde bulunmaya sistemi zorluyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11,  44,  68,\n",
       "       746, 120, 306, 432, 104, 745,  15, 194,   1,  44,  89, 744,  57,\n",
       "        20,  26, 246, 111,  11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  468,    3, 1822, 1823,  158, 1824,  209, 1825, 1826,  166,\n",
       "         52,    7, 1827,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 468,    3, 1822, 1823,  158, 1824,  209, 1825, 1826,  166,   52,\n",
       "          7, 1827,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss geçenlerde bir arkadaşımın evinin yanında durduğumda ön kapıdan çıkan arkadaşım değil fakat kocasıydı eeee'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geçenlerde bir arkadaşımın evinin yanında durduğumda ön kapıdan çıkan arkadaşım değil fakat kocasıydı eeee'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_words = len(tokenizer_src.word_index)+3\n",
    "num_decoder_words = len(tokenizer_dest.word_index)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1828"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2624"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('glove.6B.100d.txt', encoding='UTF-8') as f: #6milyar ing kelime ile eğitilmiş glove veri setini yükledik\n",
    "    for line in f:\n",
    "        values = line.split() #boşluktan ayırdık\n",
    "        word = values[0] #ilk eleman kelime\n",
    "        vec = np.asarray(values[1:], dtype='float32') #ikinci eleman 100 boyutlu kelime vektörü\n",
    "        word2vec[word] = vec #kelime ve kelime vektörünü dict. yükledik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size)) #rastgele değerlerdenoluşan giriş matrisimizi oluşturduk\n",
    "for word, i in tokenizer_src.word_index.items():\n",
    "    if i < num_encoder_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector #eğer kelime glove setinde varsa oradaki vektörü aldık yoksa rastgele değerde kaldı\n",
    "            #bu rastgele değerler eğitim esnasında hali hazırda güncellencektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1828, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None,), name='encoder_input') #y.sinir ağının giriş katmanını oluşturduk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\initializers\\initializers_v1.py:277: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding = Embedding(input_dim=num_encoder_words, #y.sinir ağınıın embedding katmanını oluşturuyoruz.\n",
    "                              output_dim=embedding_size,\n",
    "                              weights=[embedding_matrix],\n",
    "                              trainable=True, #True olduğu için kelime vektörleri eğitilecek\n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 64 #her katmandaki nöron sayısı. (Hafıza problemi varsa küçültülür [normalde 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True) #3 adet gru katmanları\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False) #son katman tekbir çıkış (düşünce vektörü) üreteceği için False yaptık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder(): #y.sinir ağı katmanlarını birbirine bağlıyoruz\n",
    "    net = encoder_input\n",
    "    \n",
    "    net = encoder_embedding(net)\n",
    "    \n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "    \n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state') #encoder çıkışını alacakak decoder girişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,), name='decoder_input') #türkçe kelimlerin girileceği decoder girişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_decoder_words, #elimizde türkçe eğitilmiş hazır kelime vektörleri bulunmadığından\n",
    "                              output_dim=embedding_size, #rastgele sayılarla vektör oluşturuyoruz. Bu vektörler zamanla eğitilecek\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True) #gpu varsa gru yerine gpu versiyonu kullanılabilir\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True) #çıkışta cümle alacağımız için son katman yine sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_words,\n",
    "                      activation='linear', #loss işleminde softmax uygulandığı için burada liear kullanıldı\n",
    "                      name='decoder_output') #vektörü kelimeye çevirmek için one-shot vektör çıkışı elde etmek için Dense katmanı ekledik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state): #katmanları birbirine bağlıyoruz\n",
    "    net = decoder_input\n",
    "    \n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "    \n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output) #düşünce vektörünü giriş olarak verdik\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output]) \n",
    "#girişi encoder ve dekoder input çıkışı decoder output olan eğitim modeli oluşturduk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output]) #test işleminde kullanılacak modelimiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state) \n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred): #loss fonksiyonumuzu uyguluyoruz\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-3) #optimizer olarak RMS kullandık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None,None)) #boyut sorunlarından dolayı placeholder kullanıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=optimizer, #model eğitimi\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'checkpoint.keras' #daha sonra kullanabilmek için eğitilen ağırlıkları kaydediyoruz\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print('Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.')\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = {'decoder_output': decoder_output_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 303 samples\n",
      "Epoch 1/10\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 7.8708\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 7.8253\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 7.6069\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 7.0307\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 4ms/sample - loss: 6.7065\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 6.5314\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 6.4052\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 6.3010\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 6.2093\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 3ms/sample - loss: 6.1296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4ec843130>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=256, #her defasında eğitilecek data miktarı\n",
    "                epochs=1, #gpu kullanılınca epoch 50'ye kadar arttırılabilir. epoch sayısı modelin üst üste kaç kez eğitilieceğini gösterir\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text, #ingilizce metni tokenlaştırdık\n",
    "                                                reverse=True,\n",
    "                                                padding='pre')\n",
    "    \n",
    "    initial_state = model_encoder.predict(input_tokens) #encoder y.sinir ağı ile düşünce vektörünü oluşturduk\n",
    "    \n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "    \n",
    "    decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n",
    "    \n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "    \n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n",
    "        \n",
    "        decoder_output = model_decoder.predict(x_data) #çeviri vektörünü elde ettik\n",
    "        \n",
    "        token_onehot = decoder_output[0, count_tokens, :] #en sondaki vektörü aldık\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        print(token_onehot)\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int) #sayısal değeri metne çevirdik\n",
    "        output_text += ' ' + sampled_word #metinleri bilrleştirip cümleyi oluşturduk\n",
    "        count_tokens += 1\n",
    "     \n",
    "    print('Input text:') #sonucu yazdırdık\n",
    "    print(input_text)\n",
    "    print()\n",
    "    \n",
    "    print('Translated text:')\n",
    "    print(output_text)\n",
    "    print()\n",
    "    \n",
    "    if true_output_text is not None:\n",
    "        print('True output text:')\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.1363475 -1.5013589  2.1221387 ... -1.342653  -1.4585345 -1.5976471]\n",
      "[ 2.4240365 -1.6492716  2.3059213 ... -1.486053  -1.5423036 -1.7766262]\n",
      "[ 2.6440752 -1.7406865  2.427619  ... -1.5786287 -1.6077179 -1.8773953]\n",
      "[ 2.8282359 -1.8007592  2.5167437 ... -1.6423247 -1.6590396 -1.931836 ]\n",
      "[ 2.9820611 -1.8418571  2.5836287 ... -1.6871872 -1.6980051 -1.9589375]\n",
      "[ 3.107643  -1.8707002  2.634133  ... -1.7190678 -1.7260998 -1.9704559]\n",
      "[ 3.2070572 -1.891034   2.6719944 ... -1.7416841 -1.7450246 -1.973461 ]\n",
      "[ 3.2833214 -1.9051745  2.6999857 ... -1.7575825 -1.7567804 -1.9721514]\n",
      "[ 3.3402383 -1.9147509  2.7203758 ... -1.7686228 -1.763405  -1.968951 ]\n",
      "[ 3.3818128 -1.9210104  2.7350504 ... -1.7762029 -1.7666708 -1.9651815]\n",
      "[ 3.4117231 -1.924925   2.745528  ... -1.7813687 -1.767927  -1.961504 ]\n",
      "[ 3.4330337 -1.9272352  2.7529762 ... -1.7848802 -1.768094  -1.9582047]\n",
      "[ 3.4481378 -1.9284862  2.7582662 ... -1.7872732 -1.7677437 -1.9553732]\n",
      "[ 3.4588196 -1.9290622  2.7620256 ... -1.7889135 -1.7672008 -1.953003 ]\n",
      "[ 3.4663744 -1.9292257  2.7647023 ... -1.7900465 -1.7666342 -1.9510468]\n",
      "[ 3.471726  -1.92915    2.7666123 ... -1.7908361 -1.7661198 -1.949446 ]\n",
      "[ 3.4755247 -1.9289461  2.7679782 ... -1.7913904 -1.7656833 -1.948141 ]\n",
      "[ 3.478226  -1.9286829  2.768955  ... -1.7917817 -1.7653254 -1.9470799]\n",
      "[ 3.4801524 -1.9284023  2.7696545 ... -1.7920582 -1.7650372 -1.946217 ]\n",
      "[ 3.4815273 -1.9281275  2.7701538 ... -1.7922539 -1.7648057 -1.9455138]\n",
      "[ 3.4825094 -1.9278711  2.770509  ... -1.7923915 -1.7646188 -1.9449403]\n",
      "[ 3.4832108 -1.9276379  2.7707593 ... -1.7924874 -1.7644657 -1.9444708]\n",
      "[ 3.48371   -1.927431   2.7709332 ... -1.7925526 -1.7643381 -1.9440852]\n",
      "[ 3.4840634 -1.9272482  2.771052  ... -1.7925959 -1.7642298 -1.943767 ]\n",
      "[ 3.4843113 -1.9270881  2.7711298 ... -1.7926232 -1.7641357 -1.943503 ]\n",
      "Input text:\n",
      "It may seem like Tom has forgotten all about the matter, but deep down inside, he's still torn up about it.\n",
      "\n",
      "Translated text:\n",
      "                                                  \n",
      "\n",
      "True output text:\n",
      "ssss Tom mesele hakkında her şeyi unutmuş gibi görünebilir fakat hâlâ gerçekten onun hakkında kendini harap ediyor. eeee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Temp\\ipykernel_10788\\1195559481.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[10], true_output_text=data_dest[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Input text:\n",
      "Which road leads to the airport?\n",
      "\n",
      "Translated text:\n",
      "                                                  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Temp\\ipykernel_10788\\1205328687.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n"
     ]
    }
   ],
   "source": [
    "translate(input_text='Which road leads to the airport?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
