{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding # CuDNNGRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss ' #başlangıç kelimesi. Bu kelime metinde geçmeyen bir kelime olmalıdır\n",
    "mark_end = ' eeee' #bitiş kelimesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = []\n",
    "data_dest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dosyadaki türkçe ve ingilizce kelimeleri ayırıyoruz. İngilizce source, Tükçe destination olarak belirlendi\n",
    "\n",
    "for line in open('tur.txt', encoding='UTF-8'):\n",
    "    en_text, tr_text = line.rstrip().split('\\t')\n",
    "    \n",
    "    tr_text = mark_start + tr_text + mark_end #türkçe hedef seti olduğu için her cümleye başlangıç ve bitiş ekleniyor\n",
    "    \n",
    "    data_src.append(en_text)\n",
    "    data_dest.append(tr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jump.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Defol. eeee'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How deep?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Ne kadar derin? eeee'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473035"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "        \n",
    "        self.fit_on_texts(texts) #kelime dictionary oluşturuyoruz\n",
    "        \n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys())) #kelime dict. tersini oluşturduk\n",
    "        \n",
    "        self.tokens = self.texts_to_sequences(texts) #kelimeleri tokenlaştırdık\n",
    "        \n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens] #ters işaretli ise listeyi tersten oluşturuyoruz\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "            \n",
    "        self.num_tokens = [len(x) for x in self.tokens] #token uzunluk dizisi oluşturduk\n",
    "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens) #max token uzunluğunu ortalam ve standart sapmadan hesapladık\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        \n",
    "        self.tokens_padded = pad_sequences(self.tokens, #padding işlemi yaparak max'tan kısa veya uzun olanları hizalayıp tüm token uzunluklarını eşitledik\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "        \n",
    "    def token_to_word(self, token): #tokenları kelimeye çeviren fonksiyon\n",
    "        word = ' ' if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "    \n",
    "    def tokens_to_string(self, tokens): #tokenlardan metin elde ediyoruz\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        text = ' '.join(words)\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, padding, reverse=False): #metinden tokenlar çıkartıyoruz\n",
    "        tokens = self.texts_to_sequences([text]) #cümleyi tokenlarına ayırıyoruz\n",
    "        tokens = np.array(tokens) \n",
    "        \n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1) #reverse ise tokenları ters çevirdik(axis 1 satır işlemi için)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "            \n",
    "        tokens = pad_sequences(tokens,\n",
    "                               maxlen=self.max_tokens,\n",
    "                               padding=padding,\n",
    "                               truncating=truncating)\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = TokenizerWrap(texts=data_src, #ingilizce metin tanınıp anlam vektörü oluşturulacağı için (encoding)\n",
    "                              padding='pre', #padding pre yani kısa olan cümlelerin başı 0 ile dolduruluyor\n",
    "                              reverse=True, #böylece y.sinir ağı sonrakileri daha etkili aldığından bu işlem yapılıyor\n",
    "                              num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dest = TokenizerWrap(texts=data_dest, #türkçe metin ise gelen ingizceden türetilmiş anlam vektöründen \n",
    "                              padding='post', #yorumlandığı için daha kısa olan türkçe kelimelerin sonu 0 ile pad ediliyor (post)\n",
    "                              reverse=False, #çünkü bu işlemin baş kısmı daha önemli\n",
    "                              num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473035, 11)\n",
      "(473035, 10)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded #pad edilmiş tokenlar olarak değişkenleri belirliyoruz\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  12, 693,   2,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss onu yap eeee'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0, 15,  9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it do'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How deep?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = tokens_src #encoder giriş için ingilizce tokenlar seçildi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = tokens_dest[:, :-1] #dekoder girişi için türkçe tokenlar seçildi\n",
    "decoder_output_data = tokens_dest[:, 1:] #dekoder çıkışı olarak ise türkçe tokenların bir basamak sonrası seçildi\n",
    "#böylece y.sinir ağı gelen türkçe kelimeden sonra gelecek kelimeyi ingilizce y.sinir ağından gelen anlam vektörü ile birlikte\n",
    "#yorumlayıp bulmaya çalışacak. Yani her defasında \n",
    "# (ing. anlam vektörü)+(türkçe kelime)=(türkçe sonraki kelime) şeklinde tahminde bulunmaya sistemi zorluyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1977])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   10,   20, 1355,    2,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,   20, 1355,    2,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss ne kadar derin eeee'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ne kadar derin eeee'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_words = len(tokenizer_src.word_index)+3\n",
    "num_decoder_words = len(tokenizer_dest.word_index)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21318"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94061"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('glove.6B.100d.txt', encoding='UTF-8') as f: #6milyar ing kelime ile eğitilmiş glove veri setini yükledik\n",
    "    for line in f:\n",
    "        values = line.split() #boşluktan ayırdık\n",
    "        word = values[0] #ilk eleman kelime\n",
    "        vec = np.asarray(values[1:], dtype='float32') #ikinci eleman 100 boyutlu kelime vektörü\n",
    "        word2vec[word] = vec #kelime ve kelime vektörünü dict. yükledik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size)) #rastgele değerlerdenoluşan giriş matrisimizi oluşturduk\n",
    "for word, i in tokenizer_src.word_index.items():\n",
    "    if i < num_encoder_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector #eğer kelime glove setinde varsa oradaki vektörü aldık yoksa rastgele değerde kaldı\n",
    "            #bu rastgele değerler eğitim esnasında hali hazırda güncellencektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21318, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None,), name='encoder_input') #y.sinir ağının giriş katmanını oluşturduk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\initializers\\initializers_v1.py:277: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding = Embedding(input_dim=num_encoder_words, #y.sinir ağınıın embedding katmanını oluşturuyoruz.\n",
    "                              output_dim=embedding_size,\n",
    "                              weights=[embedding_matrix],\n",
    "                              trainable=True, #True olduğu için kelime vektörleri eğitilecek\n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 64 #her katmandaki nöron sayısı. (Hafıza problemi varsa küçültülür [normalde 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True) #3 adet gru katmanları\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False) #son katman tekbir çıkış (düşünce vektörü) üreteceği için False yaptık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder(): #y.sinir ağı katmanlarını birbirine bağlıyoruz\n",
    "    net = encoder_input\n",
    "    \n",
    "    net = encoder_embedding(net)\n",
    "    \n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "    \n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state') #encoder çıkışını alacakak decoder girişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,), name='decoder_input') #türkçe kelimlerin girileceği decoder girişi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_decoder_words, #elimizde türkçe eğitilmiş hazır kelime vektörleri bulunmadığından\n",
    "                              output_dim=embedding_size, #rastgele sayılarla vektör oluşturuyoruz. Bu vektörler zamanla eğitilecek\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True) #gpu varsa gru yerine gpu versiyonu kullanılabilir\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True) #çıkışta cümle alacağımız için son katman yine sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_words,\n",
    "                      activation='linear', #loss işleminde softmax uygulandığı için burada liear kullanıldı\n",
    "                      name='decoder_output') #vektörü kelimeye çevirmek için one-shot vektör çıkışı elde etmek için Dense katmanı ekledik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state): #katmanları birbirine bağlıyoruz\n",
    "    net = decoder_input\n",
    "    \n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "    \n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output) #düşünce vektörünü giriş olarak verdik\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output]) \n",
    "#girişi encoder ve dekoder input çıkışı decoder output olan eğitim modeli oluşturduk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output]) #test işleminde kullanılacak modelimiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state) \n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred): #loss fonksiyonumuzu uyguluyoruz\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-3) #optimizer olarak RMS kullandık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None,None)) #boyut sorunlarından dolayı placeholder kullanıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=optimizer, #model eğitimi\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'checkpoint.keras' #daha sonra kullanabilmek için eğitilen ağırlıkları kaydediyoruz\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print('Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.')\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = {'decoder_output': decoder_output_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473035 samples\n",
      "473035/473035 [==============================] - 7678s 16ms/sample - loss: 3.8352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e25f6d120>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=256, #her defasında eğitilecek data miktarı\n",
    "                epochs=1, #gpu kullanılınca epoch 50'ye kadar arttırılabilir. epoch sayısı modelin üst üste kaç kez eğitilieceğini gösterir\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text, #ingilizce metni tokenlaştırdık\n",
    "                                                reverse=True,\n",
    "                                                padding='pre')\n",
    "    \n",
    "    initial_state = model_encoder.predict(input_tokens) #encoder y.sinir ağı ile düşünce vektörünü oluşturduk\n",
    "    \n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "    \n",
    "    decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n",
    "    \n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "    \n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n",
    "        \n",
    "        decoder_output = model_decoder.predict(x_data) #çeviri vektörünü elde ettik\n",
    "        \n",
    "        token_onehot = decoder_output[0, count_tokens, :] #en sondaki vektörü aldık\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        print(token_onehot)\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int) #sayısal değeri metne çevirdik\n",
    "        output_text += ' ' + sampled_word #metinleri bilrleştirip cümleyi oluşturduk\n",
    "        count_tokens += 1\n",
    "     \n",
    "    print('Input text:') #sonucu yazdırdık\n",
    "    print(input_text)\n",
    "    print()\n",
    "    \n",
    "    print('Translated text:')\n",
    "    print(output_text)\n",
    "    print()\n",
    "    \n",
    "    if true_output_text is not None:\n",
    "        print('True output text:')\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\ysfbil\\AppData\\Local\\Temp\\ipykernel_5552\\1195559481.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11163598 -4.4200745   1.6251723  ... -4.411964   -4.435029\n",
      " -4.438419  ]\n",
      "[-0.6494635 -3.0152292  3.8118393 ... -3.0309174 -3.0377064 -3.0382628]\n",
      "[-0.47236472 -3.7450523   6.667434   ... -3.7623637  -3.7799459\n",
      " -3.7562194 ]\n",
      "Input text:\n",
      "Jump.\n",
      "\n",
      "Translated text:\n",
      " onlar benim eeee\n",
      "\n",
      "True output text:\n",
      "ssss Defol. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[10], true_output_text=data_dest[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39887315 -4.130945   -0.23645152 ... -4.1154947  -4.1081567\n",
      " -4.123064  ]\n",
      "[ 0.37081522 -3.4878259  -0.69126093 ... -3.4909625  -3.4787383\n",
      " -3.4787087 ]\n",
      "[-1.1003442 -4.010535   1.8317606 ... -4.019283  -4.0188384 -3.9976468]\n",
      "[ 1.6391962 -3.6704965  1.9146941 ... -3.6859782 -3.6723957 -3.669701 ]\n",
      "[ 1.3815145 -3.3214421  4.8338675 ... -3.3248012 -3.3236551 -3.305161 ]\n",
      "Input text:\n",
      "Which road leads to the airport?\n",
      "\n",
      "Translated text:\n",
      " bu benim ve benim eeee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysfbil\\AppData\\Local\\Temp\\ipykernel_5552\\1195559481.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int) #boş bir decoder girişi oluşturduk\n"
     ]
    }
   ],
   "source": [
    "translate(input_text='Which road leads to the airport?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
